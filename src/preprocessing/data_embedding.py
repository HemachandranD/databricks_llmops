import logging
import mlflow.deployments
import pandas as pd
from typing import List, Iterator
from pyspark.sql.functions import explode, pandas_udf
from src.common.utility_functions import read_data_handler, write_data_with_cdc
from src.config.configuration import catalog_name, bronze_schema_name, silver_schema_name, pdf_chunks_table_name, pdf_embeddings_table_name


@pandas_udf("array<float>")
def get_embedding(contents: pd.Series) -> pd.Series:
    deploy_client = mlflow.deployments.get_deploy_client("databricks")
    def get_embeddings(batch):
        # NOTE: this will fail if an exception is thrown during embedding creation (add try/except if needed) 
        response = deploy_client.predict(endpoint="databricks-gte-large-en", inputs={"input": batch})
        return [e["embedding"] for e in response.data]

    # splitting the contents into batches of 150 items each, since the embedding model takes at most 150 inputs per request.
    max_batch_size = 150
    batches = [contents.iloc[i:i + max_batch_size] for i in range(0, len(contents), max_batch_size)]

    # process each batch and collect the results
    all_embeddings = []
    for batch in batches:
        all_embeddings += get_embeddings(batch.tolist())

    return pd.Series(all_embeddings)


if __name__ == "__main__":
    embeddings_fqn = f"{catalog_name}.{silver_schema_name}.{pdf_embeddings_table_name}"
 
    df_chunks = read_data_handler(format="del_table", schema=None, external_path=None, table_name=f"{catalog_name}.{silver_schema_name}.{pdf_chunks_table_name}")

    df_embed = (df_chunks
                .withColumn("embedding", get_embedding("content"))
                .selectExpr("pdf_name", "content", "embedding")
                )

    # if not (spark.catalog.tableExists(embed_table_name)):
    #     # Define the schema for the table
    #     schema = StructType([
    #         StructField("id", LongType(), True),  # Will be generated as IDENTITY
    #         StructField("pdf_name", StringType(), True),
    #         StructField("content", StringType(), True),
    #         StructField("embedding", ArrayType(FloatType()), True)
    #     ])

    #     # Create an empty DataFrame with the schema
    #     empty_df = spark.createDataFrame([], schema)

    #     write_data_with_cdc(empty_df, mode='append', external_path=None, table_name=embeddings_fqn)


    if not (spark.catalog.tableExists(embeddings_fqn)):
        catalog_name = embeddings_fqn.split(".")[0]
        schema_name = embeddings_fqn.split(".")[1]
        embed_table_name = embeddings_fqn.split(".")[2]

        spark.sql(f"CREATE CATALOG IF NOT EXISTS {catalog_name}")
        spark.sql(f"USE CATALOG {catalog_name}")

        spark.sql(f"CREATE SCHEMA IF NOT EXISTS {schema_name}")
        spark.sql(f"USE SCHEMA {schema_name}")
        
        spark.sql(spark.sql(f"""CREATE TABLE IF NOT EXISTS {embed_table_name}(
            id BIGINT GENERATED BY DEFAULT AS IDENTITY,
            pdf_name STRING,
            content STRING,
            embedding ARRAY <FLOAT>
            -- NOTE: the table has to be CDC because VectorSearch is using DLT that is requiring CDC state
        ) TBLPROPERTIES (delta.enableChangeDataFeed = true)"""))
    
    write_data_with_cdc(df_embed, mode='append', external_path=None, table_name=embeddings_fqn)